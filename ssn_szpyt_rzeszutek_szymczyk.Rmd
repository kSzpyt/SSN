---
title: "Sztuczne Sieci Neuronowe"
author: "Karol Szpyt, Katarzyna Rzeszutek, Agnieszka Szymczyk"
date: "16 listopada 2018"
output: html_document
---
##Wstęp
Opis problemu:
Tematem projektu jest budowa sztucznej sieci neuronowej. W naszym projekcie przeprowadzimy również badania dotyczące jej działania. 
Pierwszą czynnością będzie wybór danych. W naszym przypadku jest to zbiór obserwacji dotyczący trzech rodzajów ziaren pszenicy.

Ładujemy potrzebne biblioteki
```{r echo=TRUE, message=FALSE, warning=FALSE}
library(neuralnet)
library(nnet)
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
```
##Opis danych
Nasze dane opierają się na pomiarach właściwości geometrycznych nasion (ziaren) należących do trzech różnych odmian pszenicy. Do skonstruowania wszystkich siedmiu atrybutów o rzeczywistych wartościach zostały wykorzystana technika rentgenowska.
  
  Badaną grupę stanowiły ziarna należące do trzech różnych odmian pszenicy: **Kama**, **Rosa** i **Canadian**, każda po 70 elementów, losowo wybranych do eksperymentu. Wysokiej jakości wizualizacja wewnętrznej struktury nasiona została wykryta za pomocą techniki rentgenowskiej. Jest ona nieniszcząca (dla nasion) i znacznie tańsza niż inne, bardziej wyrafinowane techniki obrazowania, takie jak np. technologia laserowa. Obrazy rejestrowano na płytach rentgenowskich KODAK o wymiarach: 13x18 cm. Badania przeprowadzono przy użyciu ziaren pszenicy zebranych z pól doświadczalnych, zbadanych w Instytucie Agrofizyki Polskiej Akademii Nauk w Lublinie.Zestaw danych można wykorzystać do zadań klasyfikacji i analizy skupień(grupowania).

  
  Aby skonstruować dane, zmierzono siedem parametrów geometrycznych ziarna pszenicy. Atrybuty:

1.	Obszar A,
2.	Obwód B,
3.	Zwartość C = 4 \*\ pi \*\ A / P ^ 2,
4.	Długość ziarna,
5.	Szerokość ziarna,
6.	Współczynnik asymetrii,
7.	Długość rowka ziarna.
8.  Zmienna label mówiąca o typie pszenicy (1, 2, 3)
Wszystkie z powyższych parametrów były wartościowane w sposób ciągły.

Następnie wczytujemy dane, na których będziemy pracować
```{r echo=TRUE, message=FALSE, warning=FALSE}
data <- read.table("seeds.txt", sep = "\t")
names(data) <- c("area_A", 
                 "perimeter_P", 
                 "compactness_C", 
                 "length_of_kernel", 
                 "width_of_kernel", 
                 "asymmetry_coefficient", 
                 "length_of_kernel_groove", 
                 "label"
)
```
Szybkie spojrzenie na dane na poglądowych wykresach
```{r include=FALSE}
attach(data)
```

```{r fig.align='center'}
par(mfrow = c(2, 3))
plot(length_of_kernel ~ perimeter_P, col = label)
plot(compactness_C ~ length_of_kernel, col = label)
plot(width_of_kernel ~ asymmetry_coefficient, col = label)
plot(length_of_kernel_groove ~ area_A, col = label)
plot(width_of_kernel ~ perimeter_P, col = label)
plot(length_of_kernel_groove ~ asymmetry_coefficient, col = label)
```  
Ewidentnie widać podział na grupy. Sieć powinna mieć wysoką skuteczność. 

##Przygotowanie danych
Rozdzielamy zmienną "label" na 3 zmienne binarne i standaryzujemy dane, aby były wartościami od 0 do 1.  
```{r echo=TRUE, message=FALSE, warning=FALSE}
#Rozdzielanie zmiennej "label"
data2 <- cbind(data[, 1:7], class.ind(as.factor(data$label)))
names(data2) <- c(names(data)[1:7],"K", "R", "C")

#Standaryzacja danych, aby były wartościami od 0 do 1 
maximum<-apply(data2[,1:7], 2, max)
minimum<-apply(data2[,1:7], 2, min)
data3 <- as.data.frame(scale(data2[, 1:7], center = minimum,scale=maximum-minimum))
data2 <- cbind(data3, data2[,8:10])
```
Następnie dbamy o wymieszanie naszych danych.
```{r}
#Powodujemy wymieszanie danych
data2 <- data2[sample(1:210, 210),]
```
Kolejnym krokiem jest utworzenie formuły do wykorzystanie funkcji *neuralnet*.
```{r}
#tworzymy formułę
n <- names(data2)
f <- as.formula(paste("K + R + C ~", paste(n[!n %in% c("K", "R", "C")], collapse = " + ")))
f
```

```{r include=FALSE}
outs <- NULL
```


Funkcja **foo** przyjmuje nastęujące argumenty: **x** wektor, który w zależności od wymiarów oraz wartości ustawi liczbę ukrytych warst oraz liczbę neuronów w poszeczególnych warstwach. Domyślnie funkcja przyjmuje 3 neurony w pierwszej warstwie i 4 neurony w drugiej warstwie. **fct** zmienna typu character ustawia funckję aktywacji (domyślnie "logistic"). Argument **n** dotyczy walidacji krzyżowej. Walidacja wykonywana będzie zawsze w proporcjach **dane trenujące : dane testowe** (n - 1) : 1. Domyślnie jest to proporcja 5:1. Funkcja zwraca średnią skuteczność wyliczoną dzięki walidacji krzyżowej.
```{r echo=TRUE, message=FALSE, warning=FALSE}
foo <- function(x = c(3, 4), fct = "logistic", n = 6)
{
  set.seed(500)
  #walidacja krzyżowa
  for(i in 1:n)
  {
    index <- (((i-1) * round((1/n)*nrow(data2))) + 1):((i*round((1/n) * nrow(data2))))
    train_cv <- data2[index, ]
    test_cv <- data2[-index, ]
    nn_cv <- neuralnet(f,
                       data = train_cv,
                       hidden = x,
                       act.fct = fct,
                       linear.output = FALSE)
    
    
    pr.nn <- compute(nn_cv, test_cv[, 1:7])
    pr.nn_ <- pr.nn$net.result
    
    original_values <- max.col(test_cv[, 8:10])
    pr.nn_2 <- max.col(pr.nn_)
    outs[i] <- mean(pr.nn_2 == original_values)
  }
  a <- mean(outs)
  return(a)
}
```

Sprawdźmy skuteczność naszej sieci przy domyślnych parametrach
```{r echo=FALSE, message=FALSE, warning=FALSE}
foo()
```

funkcja obliczająca skutecznosć sieci w zależności od konfiguracji neuronów. Argument **y** jest tym samym argumentem co argument **x** w funckji **foo**, jednakże funckja **config** zwraca wektor skuteczności dla kolejnych kombinacji neuronóW.
```{r}
config <- function(y)
{
  a <- NULL
  for (x in 1:dim(y)[1]) {
    a[x] <- foo(y[x,])
  }
  return(a)
}
```

Sprawdzamy skuteczność sieci z jedną warstwą ukrytą. Liczba neuronóW zmienia się od 1 do 100.
```{r}
k <- NULL
for (x in 1:100) {
  k[x] <- foo(x)
}
a <- NULL
a <- c(which(k == max(k)), max(k))
names(a) <- c("liczba neuronów", "skuteczność")
a
```
```{r fig.align='center'}
plot(k, type = "l")
```

Sprawdzamy teraz zachowanie sieci przy zmianie liczby neuronów na dwóch warstwach. Z uwagi na bardzo dużą ilość kombinacji (w szczególości w następnych warstwach) liczba neuronóW na każdej z warst wahać się będzie pomiędzy 1 a 25. Ponadto wybrane zostanie losowo 10 kombinacji i na tych 10 kombinacjach sprawdzona zostanie skuteczność sieci
```{r}
set.seed(500)
kk <- expand.grid(1:25, 1:25)
ii <- sample(1:dim(kk)[1], 10)
kk <- kk[ii,]
kk <- cbind(kk, config(kk))
rownames(kk) <- 1:10
names(kk) <- c("1st", "2nd", "skuteczność")
w2 <- kk
w2
```
Ta sama sytuacja co powyżej aczkolwiek dla trzech warst
```{r}
set.seed(500)
kk <- expand.grid(1:25, 1:25, 1:25)
ii <- sample(1:dim(kk)[1], 10)
kk <- kk[ii,]
kk <- cbind(kk, config(kk))
rownames(kk) <- 1:10
names(kk) <- c("1st", "2nd", "3nd", "skuteczność")
w3 <- kk
w3
```
Dla czterech
```{r}
set.seed(500)
kk <- expand.grid(1:25, 1:25, 1:25, 1:25)
ii <- sample(1:dim(kk)[1], 10)
kk <- kk[ii,]
kk <- cbind(kk, config(kk))
rownames(kk) <- 1:10
names(kk) <- c("1st", "2nd", "3nd", "4th", "skuteczność")
w4 <- kk
w4
```
Oraz pięciu
```{r}
set.seed(500)
kk <- expand.grid(1:25, 1:25, 1:25, 1:25, 1:25)
ii <- sample(1:dim(kk)[1], 10)
kk <- kk[ii,]
kk <- cbind(kk, config(kk))
rownames(kk) <- 1:10
names(kk) <- c("1st", "2nd", "3nd", "4th", "5th", "skuteczność")
w5 <- kk
w5
```
##Skuteczność.
Przy domyślnych parametrach skuteczność sieci wynosi 90%. Jest to wysoki poziom, jednak zobaczymy, czy zwiększy się on przy zmianie neuronów lub warstw.  
Jedna warstwa.   
Przy jednej warstwie ukrytej i liczbie neuronów od 1 do 100, zauważamy wzrost skuteczności do 92,95%. 
Dwie warstwy. 
Dla dwóch warstw skuteczność waha się pomiędzy 76%, które jest najniższym wynikiem, a 92,1%.   
Trzy warstwy.  
Analogiczna sytacja, jednak dla 3 warstw. Najniższa skuteczność to 74,1%, a najwyższa to 91,43%.  
Cztery warstwy.  
Analogiczna sytuacja, 4 warstwy. Najniższa skuteczność 87,81%, a najwyższa to 91,33%.   
Pięc warstw.  
Analogiczna sytuacja, 5 warstw. Najniższa skuteczność 84,1%, a najwyższa to 90,86%.   
  
##Drzewo decyzyjne
Poniżej wyświetlone jest drzewo decyzyjne.
```{r, fig.align='center'}
tree <- rpart(label ~ ., data, method = "class")
fancyRpartPlot(tree)
```















